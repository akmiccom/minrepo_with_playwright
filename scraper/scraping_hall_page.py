from playwright.sync_api import sync_playwright
from playwright.sync_api import Page, sync_playwright, TimeoutError as PWTimeout
import pandas as pd
from urllib.parse import quote, urljoin
import re
import datetime as dt
import os

import config
from utils import _norm_text
from logger_steup import setup_logger

# =========================
# 設定・ロガー
# =========================
filename, ext = os.path.splitext(os.path.basename(__file__))
logger = setup_logger(filename, log_file=config.LOG_PATH)


# =========================
# ページ操作
# =========================
def extract_date_url(hall_url, page, period) -> list[tuple[str, str, str, str]]:
    
    """
    ホールのメインページから、直近 period 件の日付リンクを取得
    returns: List[(prefecture, h_name, date(YYYY-MM-DD), date_url)]
    """
    
    logger.info(f"ホールのメインページにアクセス: {hall_url}")
    page.goto(hall_url, timeout=90_000, wait_until="domcontentloaded")

    # ホール名・県名
    h_name = _norm_text(page.locator("#content h1").first.text_content())
    pref = _norm_text(page.locator("#content div span.todofuken").first.text_content())
    logger.info("Hall: %s / Pref: %s", h_name, pref)

    # 日付リンク
    css = "#content div table tbody tr td a"
    page.wait_for_selector(css, timeout=15_000)
    links = page.locator(css)
    count = links.count()
    logger.debug(f"link取得数: {count}")
    take = min(period, count)
    logger.debug(f"take: {take}")

    date_urls: list[tuple[str, str, str, str]] = []
    for i in range(take):
        href = links.nth(i).get_attribute("href") or ""
        date_text = _norm_text(links.nth(i).inner_text())

        # "YYYY/MM/DD" or "M/D" に対応
        m = re.match(r"(?:(\d{4})/)?(\d{1,2})/(\d{1,2})", date_text)
        if not m:
            logger.warning("日付文字列を解釈できません: %s", date_text)
            continue
        y, mth, d = m.groups()
        if y is None:
            y = str(dt.date.today().year)
        try:
            date_iso = dt.date(int(y), int(mth), int(d)).strftime("%Y-%m-%d")
        except ValueError:
            logger.warning("日付に変換できません: %s", date_text)
            continue

        date_urls.append((pref, h_name, date_iso, href))

    logger.info("取得した日付URL: %d 件", len(date_urls))
    if date_urls:
        logger.debug(f"date_urls[0] = {date_urls[0]}")

    return date_urls


if __name__ == "__main__":

    MAIN_URL = "https://min-repo.com/tag/"

    h_name = "やすだ東池袋9号店"
    h_name = "大山オーシャン"
    hall_url = MAIN_URL + quote(h_name)
    hall_url = urljoin(MAIN_URL, quote(h_name))

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        date_urls = extract_date_url(hall_url, page, period=5)
